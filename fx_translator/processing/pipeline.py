"""
ĞšĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ñ‹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ PDF Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ².

Ğ­Ñ‚Ğ¾Ñ‚ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ñ‚Ñ€Ğ¸ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ñ… ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸:
- run_pipeline() - ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ HURIDOCS ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€
- run_pipeline_transactional() - Ğ¿Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€
- run_pipeline_pymupdf() - ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ñ PyMuPDF ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼
"""

from __future__ import annotations
import os
import logging
import time
import contextlib
import requests
import tempfile
from typing import List, Dict, Tuple, Optional, Callable, Any

try:
    import pymupdf
except ImportError:
    import fitz as pymupdf  # type: ignore

from fx_translator.core.models import PageBatch, Segment
from fx_translator.core.config import (
    DEFAULT_HURIDOCS_BASE,
    HURIDOCS_ANALYZE_PATH,
    DEFAULT_LMSTUDIO_BASE,
    LMSTUDIO_MODEL,
)
from fx_translator.api.huridocs import huridocs_analyze_pdf, huridocs_analyze_pdf_smart
from fx_translator.api.lmstudio import lmstudio_translate_simple
from fx_translator.utils.text import parse_page_set
from fx_translator.utils.geometry import sort_segments_reading_order
from fx_translator.utils.metrics import init_metrics, log_metric, Timer
from fx_translator.processing.analyzers.segments import (
    refine_huridocs_segments,
    deglue_pages_pdfaware,
)
from fx_translator.processing.analyzers.layout import (
    split_spreads,
    split_spreads_force_half,
    assert_layout_invariants,
)
from fx_translator.processing.extractors.pymupdf import extract_pages_pymupdf
from fx_translator.export.docx import export_docx
from fx_translator.export.pdf import annotate_pdf_with_segments


def build_pages(seg_json: List[Dict[str, Any]]) -> List[PageBatch]:
    """
    ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµÑ‚ JSON ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· HURIDOCS Ğ² PageBatch Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹.

    Args:
        seg_json: Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² JSON Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Ğ¾Ñ‚ HURIDOCS

    Returns:
        Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº PageBatch Ñ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸, ÑĞ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼
    """
    logging.debug(f"Building pages from {len(seg_json)} segments")
    pages: Dict[int, List[Segment]] = {}

    for it in seg_json:
        seg = Segment(
            pagenumber=int(it.get("pagenumber")),
            left=float(it.get("left")),
            top=float(it.get("top")),
            width=float(it.get("width")),
            height=float(it.get("height")),
            pagewidth=float(it.get("pagewidth")),
            pageheight=float(it.get("pageheight")),
            text=str(it.get("text") or "").strip(),
            type=str(it.get("type") or "Text"),
        )
        pages.setdefault(seg.pagenumber, []).append(seg)

    batches: List[PageBatch] = []
    for pno in sorted(pages.keys()):
        segs = sort_segments_reading_order(pages[pno])
        for idx, s in enumerate(segs, start=1):
            s.blockid = idx
        batches.append(PageBatch(pagenumber=pno, segments=segs))

    return batches


def run_pipeline(
    input_pdf: str,
    out_pdf_annotated: str,
    out_docx: str,
    src_lang: str = "it",
    tgt_lang: str = "ru",
    huridocs_base: str = DEFAULT_HURIDOCS_BASE,
    huridocs_analyze_path: str = HURIDOCS_ANALYZE_PATH,
    huridocs_visualize_path: Optional[str] = None,
    lms_base: str = DEFAULT_LMSTUDIO_BASE,
    lms_model: str = LMSTUDIO_MODEL,
    batch_size: int = 15,
    force_split_spreads: bool = False,
    force_split_exceptions: str = "",
    page_limit: Optional[int] = None,
    pause_ms: int = 0,
    pause_hook: Optional[Callable[[], None]] = None,
    start_page: Optional[int] = None,
    end_page: Optional[int] = None,
    split_spreads_enabled: bool = True,
    fast=False,
) -> None:
    """
    Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ PDF Ñ‡ĞµÑ€ĞµĞ· HURIDOCS.

    Ğ­Ñ‚Ğ°Ğ¿Ñ‹:
    1. ĞĞ½Ğ°Ğ»Ğ¸Ğ· PDF Ñ‡ĞµÑ€ĞµĞ· HURIDOCS API (Ğ²ÑĞµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑÑ€Ğ°Ğ·Ñƒ)
    2. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¸ Ñ€Ğ°Ñ„Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    3. Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ² (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
    4. Deglue Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ ÑĞ»Ğ¸Ğ¿ÑˆĞ¸Ñ…ÑÑ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²
    5. ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· LM Studio
    6. Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ğ² DOCX Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ PDF

    Args:
        input_pdf: ĞŸÑƒÑ‚ÑŒ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ PDF Ñ„Ğ°Ğ¹Ğ»Ñƒ
        out_pdf_annotated: ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ PDF
        out_docx: ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ DOCX Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ¼
        src_lang: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, "en")
        tgt_lang: Ğ¦ĞµĞ»ĞµĞ²Ğ¾Ğ¹ ÑĞ·Ñ‹Ğº (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, "ru")
        huridocs_base: URL HURIDOCS API
        huridocs_analyze_path: ĞŸÑƒÑ‚ÑŒ endpoint Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
        huridocs_visualize_path: ĞŸÑƒÑ‚ÑŒ endpoint Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
        lms_base: URL LM Studio API
        lms_model: ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² LM Studio

        batch_size: Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        force_split_spreads: ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ñ‹ Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ°Ğ¼
        force_split_exceptions: Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹-Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ split (Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚: "1,3-5,10")
        page_limit: ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† (Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ)
        pause_ms: ĞŸĞ°ÑƒĞ·Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸ Ğ² Ğ¼Ñ
        pause_hook: Callback Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ°ÑƒĞ·Ñ‹
        start_page: ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° (1-based)
        end_page: ĞšĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° (1-based)
        split_spreads_enabled: Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ²
    """
    init_metrics(out_docx)

    logging.info("Ğ¨Ğ°Ğ³ 1/3: ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· HURIDOCS...")
    seg_json = huridocs_analyze_pdf(
        input_pdf, huridocs_base, huridocs_analyze_path, fast=False
    )
    pages = build_pages(seg_json)

    # ĞœÑĞ³ĞºĞ°Ñ Ğ²Ğ¾Ğ»Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ´Ğ¾ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ°
    pages = [refine_huridocs_segments(pb) for pb in pages]
    pages = deglue_pages_pdfaware(pages, pdf_path=input_pdf)

    # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†
    if start_page is not None and end_page is not None:
        pages = pages[start_page - 1 : end_page]
    elif page_limit and len(pages) > page_limit:
        pages = pages[:page_limit]

    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ²
    if split_spreads_enabled:
        total_pages = max((pb.pagenumber for pb in pages), default=0)
        if force_split_spreads:
            ex = parse_page_set(force_split_exceptions, total_pages)
            pages = split_spreads_force_half(pages, ex)
            logging.info(
                "ĞŸĞ¾ÑĞ»Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ° (force-half, Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ=%s) Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: %d.",
                sorted(list(ex)) if ex else "âˆ…",
                len(pages),
            )
        else:
            pages = split_spreads(pages, pdf_path=input_pdf, debug=True)
            logging.info("ĞŸĞ¾ÑĞ»Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ° (auto) Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: %d.", len(pages))

    # ĞœÑĞ³ĞºĞ°Ñ Ğ²Ğ¾Ğ»Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ°
    pages = [refine_huridocs_segments(pb, xtol=9.0, gaptol=10.0) for pb in pages]
    pages = deglue_pages_pdfaware(pages, pdf_path=input_pdf)

    # Ğ¨Ğ°Ğ³ 2: ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´
    logging.info("Ğ¨Ğ°Ğ³ 2/3: ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ñ‡ĞµÑ€ĞµĞ· LM Studio...")
    translations: Dict[Tuple[int, str, int], str] = {}

    for page_batch in pages:
        if pause_hook:
            pause_hook()

        segs_nonempty = [s for s in page_batch.segments if s.text.strip()]
        if not segs_nonempty:
            if pause_ms > 0:
                time.sleep(pause_ms / 1000.0)
            continue

        page_map = lmstudio_translate_simple(
            model=lms_model,
            pagenumber=page_batch.pagenumber,
            segments=segs_nonempty,
            src_lang=src_lang,
            tgt_lang=tgt_lang,
            base_url=lms_base,
        )

        side = getattr(page_batch, "logical_side", "")
        for s in segs_nonempty:
            translations[(page_batch.pagenumber, side, s.blockid)] = page_map.get(
                s.blockid, ""
            )

        if pause_ms > 0:
            time.sleep(pause_ms / 1000.0)

    # Ğ¨Ğ°Ğ³ 3: Ğ’Ñ‹Ğ²Ğ¾Ğ´
    logging.info("Ğ¨Ğ°Ğ³ 3/3: Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° (PDF + DOCX)...")
    assert_layout_invariants(pages)
    annotate_pdf_with_segments(
        input_pdf,
        out_pdf_annotated,
        pages,
        use_comments=True,  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸
        annotation_type="none",  # Ğ¡ Ğ¿Ğ¾Ğ´ÑĞ²ĞµÑ‚ĞºĞ¾Ğ¹
        include_translation=True,  # Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´
    )

    export_docx(pages, translations, out_docx, title=os.path.basename(input_pdf))

    logging.info(f"Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾: {out_pdf_annotated} Ğ¸ {out_docx}")


def analyze_pdf_transactional(
    input_pdf: str,
    huridocs_base: Optional[str] = None,
    analyze_path: str = HURIDOCS_ANALYZE_PATH,
    orchestrator: Optional[Any] = None,
    restart_every: int = 0,
    start_page: Optional[int] = None,
    end_page: Optional[int] = None,
    per_page_timeout: int = 1200,
    fast: bool = False,
) -> List[PageBatch]:
    """
    ĞŸĞ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ ÑƒĞ¼Ğ½Ñ‹Ğ¼ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ¼.

    ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ĞºĞ°Ğ¶Ğ´ÑƒÑ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾:
    - Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¾Ğ´Ğ½Ñƒ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ğ² Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ PDF
    - ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ½Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ² HURIDOCS
    - Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ° Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸

    Args:
        input_pdf: ĞŸÑƒÑ‚ÑŒ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ PDF Ñ„Ğ°Ğ¹Ğ»Ñƒ
        huridocs_base: URL HURIDOCS API
        analyze_path: ĞŸÑƒÑ‚ÑŒ endpoint Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
        orchestrator: ĞĞ±ÑŠĞµĞºÑ‚ Orchestrator Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ¼
        restart_every: ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ N ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† (0 = Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¾)
        start_page: ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° (1-based)
        end_page: ĞšĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° (1-based)
        per_page_timeout: Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ½Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹

    Returns:
        Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº PageBatch Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²
    """
    doc = pymupdf.open(input_pdf)

    try:
        total_pages = doc.page_count
        p_start = start_page or 1
        p_end = end_page or total_pages

        if not (1 <= p_start <= p_end <= total_pages):
            raise ValueError(
                f"ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {p_start}..{p_end} Ğ¸Ğ· {total_pages}"
            )

        base_url = huridocs_base or (
            f"http://localhost:{orchestrator.huridocs_port}"
            if orchestrator
            else DEFAULT_HURIDOCS_BASE
        )

        # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€ ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾
        if orchestrator:
            orchestrator.start_huridocs(lambda m: None)
            base_url = orchestrator.get_base_url()

        out_batches: List[PageBatch] = []

        for idx, pno in enumerate(range(p_start, p_end + 1), 1):
            # ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ N ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†
            if (
                orchestrator
                and restart_every > 0
                and idx > 1
                and (idx - 1) % restart_every == 0
            ):
                with contextlib.suppress(Exception):
                    orchestrator.stop_huridocs(lambda m: None)
                    orchestrator.start_huridocs(lambda m: None)
                    base_url = orchestrator.get_base_url()

            tmp_path: Optional[str] = None

            try:
                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¾Ğ´Ğ½Ñƒ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ
                page_idx = pno - 1
                page = doc[page_idx]
                pw, ph = page.rect.width, page.rect.height

                # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ PDF Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†ĞµĞ¹
                out_doc = pymupdf.open()
                out_doc.insert_pdf(doc, from_page=page_idx, to_page=page_idx)

                import tempfile

                fd, tmp_path = tempfile.mkstemp(prefix=f"page-{pno}-", suffix=".pdf")
                os.close(fd)

                out_doc.save(tmp_path, garbage=4, deflate=True)
                out_doc.close()

                # ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‡ĞµÑ€ĞµĞ· HURIDOCS
                seg_json = huridocs_analyze_pdf_smart(
                    tmp_path,
                    base_url=base_url,
                    analyze_path=analyze_path,
                    timeout=per_page_timeout,
                    fast=fast,
                )

                # ĞšĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ¾Ğ¼ĞµÑ€Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†
                for it in seg_json:
                    it["pagenumber"] = pno
                    it["pagewidth"] = pw
                    it["pageheight"] = ph

                batches = build_pages(seg_json)
                if batches:
                    out_batches.append(batches[0])

            except (requests.Timeout, requests.ConnectionError) as e:
                logging.warning(
                    f"Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {pno}: Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚/Ğ¾ÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ HURIDOCS. ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ°..."
                )
                if orchestrator and orchestrator.maybe_restart_on_failure(
                    lambda m: None, err=e
                ):
                    base_url = orchestrator.get_base_url()
                    continue

            except requests.HTTPError as e:
                status_code = getattr(e.response, "status_code", None)
                logging.warning(
                    f"Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {pno}: HTTP Ğ¾ÑˆĞ¸Ğ±ĞºĞ° {status_code} Ğ¾Ñ‚ HURIDOCS. ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞºĞ°..."
                )
                if orchestrator and orchestrator.maybe_restart_on_failure(
                    lambda m: None, status_code=status_code
                ):
                    base_url = orchestrator.get_base_url()
                    continue

            except Exception as e:
                logging.warning(f"Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° {pno}: Ğ¾Ğ±Ñ‰Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° â€” {e}")
                continue

            finally:
                if tmp_path and os.path.exists(tmp_path):
                    with contextlib.suppress(Exception):
                        os.remove(tmp_path)

        # ĞÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        if orchestrator:
            with contextlib.suppress(Exception):
                orchestrator.stop_huridocs(lambda m: None)

        return out_batches

    finally:
        doc.close()


def run_pipeline_transactional(
    input_pdf: str,
    out_pdf_annotated: str,
    out_docx: str,
    src_lang: str = "it",
    tgt_lang: str = "ru",
    huridocs_base: Optional[str] = None,
    huridocs_analyze_path: str = HURIDOCS_ANALYZE_PATH,
    lms_base: str = DEFAULT_LMSTUDIO_BASE,
    lms_model: str = LMSTUDIO_MODEL,
    batch_size: int = 15,
    force_split_spreads: bool = False,
    force_split_exceptions: str = "",
    orchestrator: Optional[Any] = None,
    restart_every: int = 0,
    start_page: Optional[int] = None,
    end_page: Optional[int] = None,
    pause_ms: int = 0,
    pause_hook: Optional[Callable[[], None]] = None,
    split_spreads_enabled: bool = True,
    fast: bool = False,
) -> None:
    """
    Ğ¢Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ¼.

    ĞÑ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ Ğ¾Ñ‚ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€Ğ°:
    - ĞšĞ°Ğ¶Ğ´Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ (analyze_pdf_transactional)
    - ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ° Ğ¿Ñ€Ğ¸ ÑĞ±Ğ¾ÑÑ…
    - ĞŸĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ N ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†
    - Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¼ÑĞ³ĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² (Ğ±ĞµĞ· Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸)

    Args:
        input_pdf: ĞŸÑƒÑ‚ÑŒ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ PDF Ñ„Ğ°Ğ¹Ğ»Ñƒ
        out_pdf_annotated: ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ PDF
        out_docx: ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ DOCX Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ¼
        src_lang: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº
        tgt_lang: Ğ¦ĞµĞ»ĞµĞ²Ğ¾Ğ¹ ÑĞ·Ñ‹Ğº
        huridocs_base: URL HURIDOCS API
        huridocs_analyze_path: ĞŸÑƒÑ‚ÑŒ endpoint Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
        lms_base: URL LM Studio API
        LMSTUDIO_MODEL: ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        batch_size: Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        force_split_spreads: ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ²
        force_split_exceptions: Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹-Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ split
        orchestrator: ĞĞ±ÑŠĞµĞºÑ‚ Orchestrator Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ¾Ğ¼
        restart_every: ĞŸĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº ĞºĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ğ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ N ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†
        start_page: ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°
        end_page: ĞšĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°
        pause_ms: ĞŸĞ°ÑƒĞ·Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸
        pause_hook: Callback Ğ´Ğ»Ñ Ğ¿Ğ°ÑƒĞ·Ñ‹
        split_spreads_enabled: Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ²
    """
    init_metrics(out_docx)

    logging.info("Ğ¨Ğ°Ğ³ 1/4: Ğ¿Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‡ĞµÑ€ĞµĞ· HURIDOCS...")
    pages = analyze_pdf_transactional(
        input_pdf=input_pdf,
        huridocs_base=huridocs_base,
        analyze_path=huridocs_analyze_path,
        orchestrator=orchestrator,
        restart_every=restart_every,
        start_page=start_page,
        end_page=end_page,
        per_page_timeout=1200,
        fast=fast,
    )

    # ĞœÑĞ³ĞºĞ°Ñ Ğ²Ğ¾Ğ»Ğ½Ğ° Ğ´Ğ¾ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ°
    pages = [refine_huridocs_segments(pb) for pb in pages]
    pages = deglue_pages_pdfaware(pages, pdf_path=input_pdf)

    # Ğ¡Ğ¿Ğ»Ğ¸Ñ‚ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ²
    if split_spreads_enabled:
        total_pages = max((pb.pagenumber for pb in pages), default=0)
        if force_split_spreads:
            ex = parse_page_set(force_split_exceptions, total_pages)
            pages = split_spreads_force_half(pages, ex)
            logging.info(
                "ĞŸĞ¾ÑĞ»Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ° (force-half, Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ=%s) Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: %d.",
                sorted(list(ex)) if ex else "âˆ…",
                len(pages),
            )
        else:
            pages = split_spreads(pages, pdf_path=input_pdf, debug=True)
            logging.info("ĞŸĞ¾ÑĞ»Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ° (auto) Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: %d.", len(pages))

    # ĞœÑĞ³ĞºĞ°Ñ Ğ²Ğ¾Ğ»Ğ½Ğ° Ğ¿Ğ¾ÑĞ»Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ°
    pages = [refine_huridocs_segments(pb, xtol=9.0, gaptol=10.0) for pb in pages]
    pages = deglue_pages_pdfaware(pages, pdf_path=input_pdf)

    # ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´ (Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ñ‹)
    logging.info("Ğ¨Ğ°Ğ³ 3/4: Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· LM Studio...")
    translations: Dict[Tuple[int, str, int], str] = {}

    for page_batch in pages:
        if pause_hook:
            pause_hook()

        def _for_translation(s: Segment) -> bool:
            t = (s.text or "").strip()

            if not t:
                return False

            # âœ… ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ’Ğ¡Ğ• Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¸
            if s.type in ("title", "section_header", "caption", "page_header"):
                return True

            # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ¾Ğ¼ĞµÑ€Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†
            if t.isdigit() and s.type == "page_footer":
                return False

            # ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ²ÑÑ‘ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ
            return True

        segs_for_translation = [s for s in page_batch.segments if _for_translation(s)]

        page_map = lmstudio_translate_simple(
            model=lms_model,
            pagenumber=page_batch.pagenumber,
            segments=segs_for_translation,
            src_lang=src_lang,
            tgt_lang=tgt_lang,
            base_url=lms_base,
        )

        side = getattr(page_batch, "logical_side", "")
        for s in segs_for_translation:
            logging.info(
                f"Block {s.blockid}: original text = '{s.text}' (type: {s.type})"
            )
            translations[(page_batch.pagenumber, side, s.blockid)] = page_map.get(
                s.blockid, ""
            )

        if pause_ms > 0:
            time.sleep(pause_ms / 1000.0)

    # Ğ’Ñ‹Ğ²Ğ¾Ğ´
    logging.info("Ğ¨Ğ°Ğ³ 4/4: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ PDF Ğ¸ DOCX...")
    assert_layout_invariants(pages)
    annotate_pdf_with_segments(
        input_pdf,
        out_pdf_annotated,
        pages,
        use_comments=True,  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸
        annotation_type="none",  # Ğ¡ Ğ¿Ğ¾Ğ´ÑĞ²ĞµÑ‚ĞºĞ¾Ğ¹
        include_translation=True,  # Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´
    )
    # ĞÑ‚Ğ»Ğ°Ğ´Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´
    logging.info(f"Total translations: {len(translations)}")

    # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼
    from collections import defaultdict

    by_page = defaultdict(dict)

    for (pno, side, blockid), trans_text in translations.items():
        by_page[(pno, side)][blockid] = trans_text

    # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼
    for (pno, side), trans_dict in by_page.items():
        logging.info(f"Page ({pno}, {side}): {len(trans_dict)} translations")
        for blockid, trans_text in trans_dict.items():
            logging.info(f"  Block {blockid}: {trans_text[:50]}...")

    export_docx(pages, translations, out_docx, title=os.path.basename(input_pdf))


def featurize_segments_for_llm(pb: PageBatch) -> Dict[str, Any]:
    """
    Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°ĞºÑ‚Ğ½Ñ‹Ğ¹ JSON-Ğ¿ĞµĞ¹Ğ»Ğ¾Ğ°Ğ´ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾Ğ¹ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ LLM.

    Args:
        pb: PageBatch Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸

    Returns:
        Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ LLM
    """
    feats = []
    for s in sort_segments_reading_order(pb.segments):
        feats.append(
            {
                "blockid": s.blockid,
                "bbox": [s.left, s.top, s.left + s.width, s.top + s.height],
                "type": s.type,
                "text": (s.text or "")[:400],  # Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ¼ Ğ´Ğ»Ğ¸Ğ½Ñƒ
            }
        )
    return {"pagenumber": pb.pagenumber, "segments": feats}


def llm_group_segments(
    model: str, lms_base: str, page_payload: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹.

    ĞŸĞ¾Ğ·Ğ¶Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ² LM Studio Ğ´Ğ»Ñ
    Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· LLM.

    Args:
        model: ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LM Studio
        lms_base: Base URL LM Studio API
        page_payload: Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¾Ñ‚ featurize_segments_for_llm()

    Returns:
        Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ°Ğ¼Ğ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²
    """
    return {
        "groups": [
            {"blockid": it["blockid"], "type": it.get("type", "paragraph")}
            for it in page_payload.get("segments", [])
        ]
    }


def apply_llm_groups(pb: PageBatch, grouping: Dict[str, Any]) -> PageBatch:
    """
    ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ Ñ‚Ğ¸Ğ¿Ñ‹ Ğ¸Ğ· Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ LLM Ğº ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ğ¼.

    Args:
        pb: PageBatch Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ
        grouping: Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¾Ñ‚ llm_group_segments()

    Returns:
        ĞĞ±Ğ½Ğ¾Ğ²Ğ»Ñ‘Ğ½Ğ½Ñ‹Ğ¹ PageBatch
    """
    by_id = {s.blockid: s for s in pb.segments}

    for g in grouping.get("groups", []):
        bid = int(g.get("blockid", 0))
        new_type = str(g.get("type", "")).strip()
        if bid in by_id and new_type:
            by_id[bid].type = new_type

    # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº/Ğ½ÑƒĞ¼ĞµÑ€Ğ°Ñ†Ğ¸Ñ
    segs = sort_segments_reading_order(list(by_id.values()))
    for i, s in enumerate(segs, 1):
        s.blockid = i

    return PageBatch(
        pagenumber=pb.pagenumber,
        segments=segs,
        logical_side=getattr(pb, "logical_side", ""),
    )


def run_pipeline_pymupdf(
    input_pdf: str,
    out_pdf_annotated: str,
    out_docx: str,
    src_lang: str = "en",
    tgt_lang: str = "ru",
    lms_base: str = DEFAULT_LMSTUDIO_BASE,
    lms_model: str = LMSTUDIO_MODEL,
    batch_size: int = 15,
    split_spreads_enabled: bool = True,
    force_split_spreads: bool = False,
    force_split_exceptions: str = "",
    start_page: Optional[int] = None,
    end_page: Optional[int] = None,
    use_llm_grouping: bool = False,
    pause_ms: int = 0,
    pause_hook: Optional[Callable[[], None]] = None,
) -> None:
    """
    ĞšĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ñ PyMuPDF ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€Ğ¾Ğ¼ (Ğ±ĞµĞ· HURIDOCS).

    Ğ­Ñ‚Ğ°Ğ¿Ñ‹:
    1. Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· PyMuPDF AdvancedTextProcessor
    2. Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ² (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
    3. LLM-Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ‚Ğ¸Ğ¿Ğ¾Ğ² (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
    4. ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· LM Studio
    5. Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ğ² DOCX Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ PDF

    Args:
        input_pdf: ĞŸÑƒÑ‚ÑŒ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ PDF Ñ„Ğ°Ğ¹Ğ»Ñƒ
        out_pdf_annotated: ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ PDF
        out_docx: ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ DOCX Ñ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¾Ğ¼
        src_lang: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº
        tgt_lang: Ğ¦ĞµĞ»ĞµĞ²Ğ¾Ğ¹ ÑĞ·Ñ‹Ğº
        lms_base: URL LM Studio API
        LMSTUDIO_MODEL: ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        batch_size: Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        split_spreads_enabled: Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ²
        force_split_spreads: ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ²
        force_split_exceptions: Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹-Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ split
        start_page: ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° (1-based)
        end_page: ĞšĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° (1-based)
        use_llm_grouping: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ LLM Ğ´Ğ»Ñ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²
        pause_ms: ĞŸĞ°ÑƒĞ·Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸
        pause_hook: Callback Ğ´Ğ»Ñ Ğ¿Ğ°ÑƒĞ·Ñ‹
    """
    init_metrics(out_docx)

    logging.info("PyMuPDF: ÑˆĞ°Ğ³ 1/4 â€” Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ ÑĞ±Ğ¾Ñ€ĞºĞ° Ğ°Ğ±Ğ·Ğ°Ñ†ĞµĞ²...")
    pages = extract_pages_pymupdf(input_pdf, start_page=start_page, end_page=end_page)
    logging.info("Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¾ %d ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† (Ğ´Ğ¾ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ°).", len(pages))

    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ²
    if split_spreads_enabled:
        if force_split_spreads:
            total_pages = max((pb.pagenumber for pb in pages), default=0)
            ex = parse_page_set(force_split_exceptions, total_pages)
            pages = split_spreads_force_half(pages, ex)
            logging.info(
                "ĞŸĞ¾ÑĞ»Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ° (force-half, Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ=%s) Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: %d.",
                sorted(list(ex)) if ex else "âˆ…",
                len(pages),
            )
        else:
            pages = split_spreads(pages, pdf_path=input_pdf, debug=True)
            logging.info("ĞŸĞ¾ÑĞ»Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ° (auto) Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: %d.", len(pages))

    # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ LLM-Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ€Ğ¾Ğ»ĞµĞ¹ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
    if use_llm_grouping:
        logging.info("PyMuPDF: ÑˆĞ°Ğ³ 2/4 â€” LLM-Ğ³Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ€Ğ¾Ğ»ĞµĞ¹ Ğ¸ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²...")
        grouped: List[PageBatch] = []
        for pb in pages:
            try:
                payload = featurize_segments_for_llm(pb)
                grouping = llm_group_segments(
                    model=lms_model, lms_base=lms_base, page_payload=payload
                )
                grouped.append(apply_llm_groups(pb, grouping))
            except Exception as e:
                logging.warning(
                    f"LLM grouping failed on page {pb.pagenumber}: {e} â€” using passthrough"
                )
                grouped.append(pb)
        pages = grouped

    # ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´
    logging.info("PyMuPDF: ÑˆĞ°Ğ³ 3/4 â€” Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· LM Studio...")
    translations: Dict[Tuple[int, str, int], str] = {}

    for pb in pages:
        if pause_hook:
            pause_hook()

        segs = [s for s in pb.segments if s.text.strip()]
        if not segs:
            if pause_ms > 0:
                time.sleep(pause_ms / 1000.0)
            continue

        page_map = lmstudio_translate_simple(
            model=lms_model,
            pagenumber=pb.pagenumber,
            segments=segs,
            src_lang=src_lang,
            tgt_lang=tgt_lang,
            base_url=lms_base,
        )

        side = getattr(pb, "logical_side", "")
        for s in segs:
            translations[(pb.pagenumber, side, s.blockid)] = page_map.get(s.blockid, "")

        if pause_ms > 0:
            time.sleep(pause_ms / 1000.0)

    # Ğ’Ñ‹Ğ²Ğ¾Ğ´
    logging.info("PyMuPDF: ÑˆĞ°Ğ³ 4/4 â€” Ğ²Ñ‹Ğ¿ÑƒÑĞº Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ PDF Ğ¸ DOCX...")
    assert_layout_invariants(pages)
    annotate_pdf_with_segments(
        input_pdf,
        out_pdf_annotated,
        pages,
        use_comments=True,  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸
        annotation_type="none",  # Ğ¡ Ğ¿Ğ¾Ğ´ÑĞ²ĞµÑ‚ĞºĞ¾Ğ¹
        include_translation=True,  # Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´
    )
    export_docx(pages, translations, out_docx, title=os.path.basename(input_pdf))

    logging.info("Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾: %s Ğ¸ %s", out_pdf_annotated, out_docx)


def run_pipeline_layoutlmv3(
    input_pdf: str,
    out_pdf_annotated: str,
    out_docx: str,
    src_lang: str = "it",
    tgt_lang: str = "ru",
    lms_base: str = DEFAULT_LMSTUDIO_BASE,
    lms_model: str = LMSTUDIO_MODEL,
    start_page: Optional[int] = None,
    end_page: Optional[int] = None,
    split_spreads_enabled: bool = True,
    force_split_spreads: bool = False,
    force_split_exceptions: str = "",
    use_gpu: bool = True,
    dpi: int = 200,
    pause_ms: int = 0,
    pause_hook: Optional[Callable[[], None]] = None,
) -> None:
    """
    ĞšĞ¾Ğ½Ğ²ĞµĞ¹ĞµÑ€ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ PDF Ñ‡ĞµÑ€ĞµĞ· LayoutLMv3.

    Ğ­Ñ‚Ğ°Ğ¿Ñ‹:
    1. ĞĞ½Ğ°Ğ»Ğ¸Ğ· PDF Ñ‡ĞµÑ€ĞµĞ· LayoutLMv3 (Ğ¿Ğ¾ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ GPU)
    2. ĞŸĞ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² (refine + deglue)
    3. Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ² (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
    4. ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· LM Studio
    5. Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ğ² DOCX Ğ¸ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ PDF

    Args:
        input_pdf: ĞŸÑƒÑ‚ÑŒ Ğº Ğ²Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ PDF
        out_pdf_annotated: ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ Ğ°Ğ½Ğ½Ğ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ PDF
        out_docx: ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ DOCX
        src_lang: Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, "it")
        tgt_lang: Ğ¦ĞµĞ»ĞµĞ²Ğ¾Ğ¹ ÑĞ·Ñ‹Ğº (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, "ru")
        lms_base: URL LM Studio API
        lms_model: ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ LM Studio
        start_page: ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° (1-based)
        end_page: ĞšĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° (1-based)
        split_spreads_enabled: Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ²
        force_split_spreads: ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ°Ğ¼
        force_split_exceptions: Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹-Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ split
        use_gpu: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ GPU Ğ´Ğ»Ñ LayoutLMv3
        dpi: DPI Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ PDF â†’ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
        pause_ms: ĞŸĞ°ÑƒĞ·Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°Ğ¼Ğ¸ (Ğ¼Ñ)
        pause_hook: Callback Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ°ÑƒĞ·Ñ‹
    """
    from fx_translator.api.layoutlmv3 import LayoutLMv3Analyzer

    init_metrics(out_docx)

    logging.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logging.info("ğŸš€ LayoutLMv3 Pipeline")
    logging.info(f"   PDF: {input_pdf}")
    logging.info(f"   DPI: {dpi}")
    logging.info(f"   GPU: {'Ğ’ĞºĞ»ÑÑ‡ĞµĞ½Ğ¾' if use_gpu else 'ĞÑ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¾'}")
    logging.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    # Ğ¨Ğ°Ğ³ 1: ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ‡ĞµÑ€ĞµĞ· LayoutLMv3
    logging.info("Ğ¨Ğ°Ğ³ 1/4: ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ°ĞºĞµÑ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· LayoutLMv3...")

    analyzer = LayoutLMv3Analyzer(
        model_name="microsoft/layoutlmv3-large", use_gpu=use_gpu
    )

    seg_json = analyzer.analyze_pdf(
        pdf_path=input_pdf,
        dpi=dpi,
        start_page=start_page,
        end_page=end_page,
    )

    logging.info(f"ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¾ {len(seg_json)} ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¾Ñ‚ LayoutLMv3")

    # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ Ğ² PageBatch
    pages = build_pages(seg_json)
    logging.info(f"Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ {len(pages)} ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†")

    # Ğ¨Ğ°Ğ³ 2: ĞŸĞ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° (Ğ¼ÑĞ³ĞºĞ°Ñ Ğ²Ğ¾Ğ»Ğ½Ğ°)
    logging.info("Ğ¨Ğ°Ğ³ 2/4: ĞŸĞ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²...")

    # ĞŸĞµÑ€Ğ²Ğ°Ñ Ğ²Ğ¾Ğ»Ğ½Ğ° - Ğ¼ÑĞ³ĞºĞ¾Ğµ ÑƒÑ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ
    pages = [refine_huridocs_segments(pb, xtol=3.0, gaptol=4.0) for pb in pages]

    # Deglue Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ ÑĞ»Ğ¸Ğ¿ÑˆĞ¸Ñ…ÑÑ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ²
    pages = deglue_pages_pdfaware(pages, pdf_path=input_pdf)

    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ¾Ğ²
    if split_spreads_enabled:
        total_pages = max((pb.pagenumber for pb in pages), default=0)

        if force_split_spreads:
            ex = parse_page_set(force_split_exceptions, total_pages)
            pages = split_spreads_force_half(pages, ex)
            logging.info(
                f"ĞŸĞ¾ÑĞ»Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ° (force-half, Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ={sorted(list(ex)) if ex else 'âˆ…'}) "
                f"Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {len(pages)}"
            )
        else:
            pages = split_spreads(pages, pdf_path=input_pdf, debug=True)
            logging.info(f"ĞŸĞ¾ÑĞ»Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ° (auto) Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {len(pages)}")

    # Ğ’Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ²Ğ¾Ğ»Ğ½Ğ° Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ¿Ğ¾ÑĞ»Ğµ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ°
    pages = [refine_huridocs_segments(pb, xtol=3.0, gaptol=4.0) for pb in pages]
    pages = deglue_pages_pdfaware(pages, pdf_path=input_pdf)

    # Ğ¨Ğ°Ğ³ 3: ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´
    logging.info("Ğ¨Ğ°Ğ³ 3/4: ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· LM Studio...")
    translations: Dict[Tuple[int, str, int], str] = {}

    for page_batch in pages:
        if pause_hook:
            pause_hook()

        segs_nonempty = [s for s in page_batch.segments if s.text.strip()]
        if not segs_nonempty:
            if pause_ms > 0:
                time.sleep(pause_ms / 1000.0)
            continue

        page_map = lmstudio_translate_simple(
            model=lms_model,
            pagenumber=page_batch.pagenumber,
            segments=segs_nonempty,
            src_lang=src_lang,
            tgt_lang=tgt_lang,
            base_url=lms_base,
        )

        side = getattr(page_batch, "logical_side", "")
        for s in segs_nonempty:
            translations[(page_batch.pagenumber, side, s.blockid)] = page_map.get(
                s.blockid, ""
            )

        if pause_ms > 0:
            time.sleep(pause_ms / 1000.0)

    # Ğ¨Ğ°Ğ³ 4: Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚
    logging.info("Ğ¨Ğ°Ğ³ 4/4: Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ° (PDF + DOCX)...")
    assert_layout_invariants(pages)

    annotate_pdf_with_segments(
        input_pdf,
        out_pdf_annotated,
        pages,
        use_comments=True,
        annotation_type="none",
        include_translation=True,
    )

    export_docx(pages, translations, out_docx, title=os.path.basename(input_pdf))

    logging.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logging.info(f"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾!")
    logging.info(f"   PDF: {out_pdf_annotated}")
    logging.info(f"   DOCX: {out_docx}")
    logging.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
